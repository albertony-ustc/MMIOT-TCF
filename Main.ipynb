{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***注意事项*：\n",
    "- 本文件运行位置：./\n",
    "- 本文件所属文件夹下内容：\n",
    "    1. Data：LSH模型所需测试集与训练集\n",
    "    2. QueryResult：LSH查询测试集产生的查询结果\n",
    "    3. PingPong：NDSS 2020 PingPong数据集\n",
    "    4. CsvHub：五元组(Quintuple), 流量特征文件(Features)\n",
    "    5. device_ip_map.json：IOT设备的IP集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 基于流包长分布、局部敏感性哈希算法的事件级指纹识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### （一）原始流量基于五元组的汇编\n",
    "- 内容概述：五元组（源IP地址、目的IP地址、协议号、源端口、目的端口）用于描述每一个数据包的基本信息。需要将五元组聚合城"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''五元组流量汇编模块\n",
    "   输入：PingPong Pcap数据集（含测试集、训练集）\n",
    "   输出：csv 五元组汇编文件，csv 流量特征文件\n",
    "'''\n",
    "\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import io\n",
    "import struct\n",
    "from tqdm.notebook import tqdm  #展示进度条\n",
    "import binascii\n",
    "\n",
    "error_pcap_name = []  #错误统计\n",
    "command_Quintuple = \"nohup tshark -r {} -T fields -E separator=, -e frame.number -e frame.time_epoch -e frame.len -e eth.src -e eth.dst -e ip.src -e ip.dst -e ip.proto >> {} &\"\n",
    "command_Features = 'tshark -r {} -T fields -E separator=$ -e frame.number -e frame.time_epoch -e eth.type -e ip.src -e ip.dst -e ip.proto -e ip.opt.padding -e ip.opt.ra -e tcp.srcport -e tcp.dstport -e tcp.stream -e tcp.window_size -e tcp.len -e ssl.handshake.ciphersuite -e udp.srcport -e udp.dstport -e udp.stream -e dns.qry.name -e http -e ntp > {}'\n",
    "\n",
    "#1. 文件预统计\n",
    "path = r'./PingPong'    #PingPong数据集路径\n",
    "count = 0\n",
    "for root,dirs,files in os.walk(path):    #遍历统计\n",
    "    for each in files:\n",
    "        if each.endswith(\".pcap\"):\n",
    "            count += 1  #统计文件夹下文件个数\n",
    "print(f\"数据集Pcap文件 共计：{count}(个)\")  # 输出结果\n",
    "ShowProcessBar = tqdm(range(count))\n",
    "\n",
    "#2. Pcap文件处理\n",
    "for dir,folder,file in os.walk(path):\n",
    "    for f in file:\n",
    "        #剔除非pcap文件\n",
    "        if (f[-5:] != \".pcap\"):\n",
    "            continue  #跳过\n",
    "        \n",
    "        file_path = f'{dir}/'+f\n",
    "        ShowProcessBar.update(1)  #更新进度条\n",
    "        \n",
    "        try: \n",
    "            #五元组提取\n",
    "            os.system(command_Quintuple.format(file_path,file_path+'.Quintuple.csv'))\n",
    "            #流量特征提取\n",
    "            os.system(command_Features.format(file_path,file_path+'.features.csv'))  #生成features.csv\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"**错误：处理此文件时发生异常{dir}/{f}\")\n",
    "            print('错误信息：'+str(e))\n",
    "            error_pcap_name.append(f'{dir}/'+f)\n",
    "            try:\n",
    "                os.remove(f'{dir}/'+f+'.Quintuple.csv')\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "print('五元组提取完毕！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''构建CsvHub(1,五元组部分)'''\n",
    "# 五元组汇编文件（训练/测试集分类）\n",
    "# 流量特征文件\n",
    "for root,dirs,file in os.walk('./PingPong/'):\n",
    "    for each in file:\n",
    "        if (each.find('detection') != -1 and each.endswith('.csv')):\n",
    "            os.system(f'cp {root}/{each} ./CsvHub/Quintuple/test-data/')\n",
    "        elif (each.find('detection') == -1 and each.endswith('.csv')):\n",
    "            os.system(f'cp {root}/{each} ./CsvHub/Quintuple/train-data/')\n",
    "        elif (each.endswith('.features.csv')):\n",
    "            os.system(f'cp {root}/{each} ./CsvHub/Features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （二）报文长度概率分布特征向量的提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iot设备IP-MAC地址的提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''iot设备IP地址提取模块'''\n",
    "\n",
    "# 从json中提取设备IP地址\n",
    "f = open('./device_ip_map.json','r')\n",
    "eval(f.read())\n",
    "\n",
    "# 提取每个iot设备的ip（从csv文件中）\n",
    "DevciesToIp = {}   #iot设备五元组文件(.csv)所对应的ip地址\n",
    "Path = r'./PingPong'    \n",
    "\n",
    "'''函数：提取timestamps文件所对应的五元组文件列表'''\n",
    "def WalkDirCsv(Path):\n",
    "    FileList = []\n",
    "    for root,dirs,files in os.walk(Path):\n",
    "        for each in files:\n",
    "            if each.endswith(\"Quintuple.csv\"):\n",
    "                FileList.append(root + '/' + each)\n",
    "    \n",
    "    return FileList\n",
    "                \n",
    "#提取timestamps对应的csv文件\n",
    "for root,dirs,files in os.walk(Path):    \n",
    "    for each in files:\n",
    "        if each.endswith(\".timestamps\"):\n",
    "            if (each in devices_ip):   #剔除无ip记录的五元组文件\n",
    "                FList = WalkDirCsv(root.split('/timestamps')[0])\n",
    "                \n",
    "                for File in FList:\n",
    "                    DevciesToIp[File] = devices_ip[each]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning：文件sengled-bulb-intensity.eth1.local.pcap.Quintuple.csv，IP未找到。\n",
      "\n",
      "MAC地址表：\n",
      "dlink-plug        90:8d:78:e3:81:0c\n",
      "dlink-siren        c4:12:f5:de:38:20\n",
      "hue-bulb-intensity        00:17:88:69:ee:e4\n",
      "hue-bulb-onoff        00:17:88:69:ee:e4\n",
      "rachio-sprinkler-quickrun        d0:c5:d3:28:c9:2a\n",
      "tplink-bulb-color        50:c7:bf:59:d5:84\n",
      "tplink-bulb-intensity        50:c7:bf:59:d5:84\n",
      "tplink-bulb-onoff        50:c7:bf:59:d5:84\n",
      "tplink-plug        50:c7:bf:33:1f:09\n",
      "wemo-insight-plug        14:91:82:25:10:77\n",
      "wemo-plug        94:10:3e:36:60:09\n",
      "amazon-plug        5c:41:5a:ce:36:02\n",
      "arlo-camera        64:bc:0c:43:3f:40\n",
      "blossom-sprinkler-mode        64:bc:0c:43:3f:40\n",
      "blossom-sprinkler-quickrun        28:c2:dd:47:17:b6\n",
      "dlink-plug        90:8d:78:e3:81:0c\n",
      "dlink-siren        64:bc:0c:43:3f:40\n",
      "ecobee-thermostat-fan        64:bc:0c:2a:d2:aa\n",
      "ecobee-thermostat-hvac        64:bc:0c:2a:d2:aa\n",
      "kwikset-doorlock        64:bc:0c:43:3f:40\n",
      "nest-thermostat        64:bc:0c:43:3f:40\n",
      "rachio-sprinkler-mode        d0:c5:d3:28:c9:2a\n",
      "rachio-sprinkler-quickrun        d0:c5:d3:28:c9:2a\n",
      "ring-alarm        f4:84:4c:45:69:07\n",
      "roomba-vacuum-robot        64:bc:0c:43:3f:40\n",
      "sengled-bulb-intensity        64:bc:0c:43:3f:40\n",
      "sengled-bulb-onoff        64:bc:0c:43:3f:40\n",
      "sengled-bulb-onoff        64:bc:0c:43:3f:40\n",
      "st-plug        64:bc:0c:43:3f:40\n",
      "tplink-bulb-color        64:bc:0c:43:3f:40\n",
      "tplink-bulb-intensity        64:bc:0c:43:3f:40\n",
      "tplink-bulb-onoff        64:bc:0c:43:3f:40\n",
      "tplink-plug        50:c7:bf:33:1f:09\n",
      "wemo-insight-plug        64:bc:0c:43:3f:40\n",
      "wemo-plug        64:bc:0c:43:3f:40\n",
      "blink-camera-photo        f4:b8:5e:fe:19:bc\n",
      "blink-camera-watch        f4:b8:5e:fe:19:bc\n",
      "amazon-plug        5c:41:5a:ce:36:02\n",
      "dlink-plug        90:8d:78:e3:81:0c\n",
      "rachio-sprinkler-mode        d0:c5:d3:28:c9:2a\n",
      "rachio-sprinkler-quickrun        d0:c5:d3:28:c9:2a\n",
      "ring-alarm        f4:84:4c:45:69:07\n",
      "tplink-plug        50:c7:bf:33:1f:09\n",
      "tplink-bulb-white-intensity        d8:0d:17:33:c2:52\n",
      "tplink-bulb-white-onoff        d8:0d:17:33:c2:52\n",
      "tplink-camera-onoff        50:d4:f7:b9:d2:25\n",
      "tplink-power-strip        50:d4:f7:d5:80:7a\n",
      "tplink-two-outlet-plug        68:ff:7b:72:98:44\n"
     ]
    }
   ],
   "source": [
    "'''iot设备MAC地址提取模块'''\n",
    "\n",
    "DevicesMAC = {}\n",
    "\n",
    "'''函数：常规匹配五元组文件(.csv)中的IP与MAC地址'''\n",
    "def FindMAC(FilePath):\n",
    "#读取设备csv\n",
    "    with open(FilePath,'r') as f:\n",
    "        while(True):\n",
    "            EventPacket = f.readline().split(',')\n",
    "            if (len(EventPacket) == 1):\n",
    "                f.close()\n",
    "                raise Exception(f'文件:{FilePath},\\n 未找到设备对应的IP信息!')\n",
    "            \n",
    "            #寻找IP与MAC关系\n",
    "            if (EventPacket[5] == DevciesToIp[FilePath]):\n",
    "                f.close()\n",
    "                return EventPacket[3]\n",
    "            elif (EventPacket[6] == DevciesToIp[FilePath]):\n",
    "                f.close()\n",
    "                return EventPacket[4] \n",
    "        \n",
    "                            \n",
    "for root,dirs,files in os.walk(Path):    #遍历统计\n",
    "    for each in files:\n",
    "        FilePath = root + '/' + each\n",
    "        \n",
    "        if each.endswith(\".Quintuple.csv\"):\n",
    "            if (FilePath in DevciesToIp):   #文件有对应设备的IP\n",
    "                try:\n",
    "                    DevicesMAC[each] = FindMAC(FilePath)  #找到对应MAC\n",
    "                except:\n",
    "                    print(f'Warning：文件{each}，IP未找到。')\n",
    "\n",
    "print('\\nMAC地址表：')\n",
    "for i in DevicesMAC:\n",
    "    print(i.split('.')[0],'      ',DevicesMAC[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*特殊IP(MAC)地址处理\n",
    "非IOT设备的IP(MAC)地址，如路由器、网关、智能手机等，设计算法与手工结合的方式，选取非此类设备的、在同一设备数据包中出现次数最多的IP(MAC)地址，作为该设备的地址。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 概率分布特征向量的提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 数据集（训练、测试集）的构造与生成模块 '''\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DIRECTION_IN = 0x00\n",
    "DIRECTION_OUT = 0x01\n",
    "\n",
    "\n",
    "class UNSWDataset(object):\n",
    "    # 待识别的iot设备MAC地址集(提取好的)\n",
    "    DEVICE_IOT_LIST = '''\n",
    "        kwikset-doorlock          90:8d:78:e3:81:0c\n",
    "        nest-thermostat          64:16:66:1f:f0:3e\n",
    "        roomba-vacuum-robot          d0:c5:d3:90:54:73\n",
    "        sengled-bulb-onoff          b0:ce:18:27:4d:ab\n",
    "        tplink-bulb-color          50:c7:bf:59:d5:84\n",
    "        dlink-plug          90:8d:78:e3:81:0c\n",
    "        dlink-siren          c4:12:f5:de:38:20\n",
    "        hue-bulb          00:17:88:69:ee:e4\n",
    "        rachio-sprinkler          d0:c5:d3:28:c9:2a\n",
    "        tplink-plug          50:c7:bf:33:1f:09\n",
    "        wemo-insight-plug          14:91:82:23:cc:41\n",
    "        wemo-plug          94:10:3e:36:60:09\n",
    "        amazon-plug          5c:41:5a:ce:36:02\n",
    "        blossom-sprinkler          28:c2:dd:47:17:b6\n",
    "        ecobee-thermostat          64:bc:0c:2a:d2:aa\n",
    "        ring-alarm          f4:84:4c:45:69:07\n",
    "        blink-camera-photo          f4:b8:5e:fe:19:bc\n",
    "    '''.split()\n",
    "    \n",
    "    # 非iot设备的MAC地址集（智能终端等）\n",
    "    DEVICE_NON_IOT_LIST = '''\n",
    "    '''.split()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device_list, self.iot_list, self.non_iot_list = {}, {}, {}\n",
    "        for i in range(0, len(UNSWDataset.DEVICE_IOT_LIST), 2):\n",
    "            self.iot_list[UNSWDataset.DEVICE_IOT_LIST[i]] = UNSWDataset.DEVICE_IOT_LIST[i + 1]\n",
    "            self.device_list[UNSWDataset.DEVICE_IOT_LIST[i]] = UNSWDataset.DEVICE_IOT_LIST[i + 1]\n",
    "        for i in range(0, len(UNSWDataset.DEVICE_NON_IOT_LIST), 2):\n",
    "            self.non_iot_list[UNSWDataset.DEVICE_NON_IOT_LIST[i]] = \\\n",
    "                UNSWDataset.DEVICE_NON_IOT_LIST[i + 1]\n",
    "            self.device_list[UNSWDataset.DEVICE_NON_IOT_LIST[i]] = \\\n",
    "                UNSWDataset.DEVICE_NON_IOT_LIST[i + 1]\n",
    "        self.mac_device_map = {v: k for k, v in self.device_list.items()}\n",
    "        self.label_map = {mac: i + 1 for i, mac in enumerate(self.mac_device_map.keys())}\n",
    "        \n",
    "        #获取所有csv文件列表\n",
    "        self.FEATURE_PATH = './CsvHub/Features'\n",
    "\n",
    "    '''函数：读取五元组与流数据'''\n",
    "    def data_generator(self,Path):\n",
    "        self.FILE_PATH = os.listdir(Path)\n",
    "        \n",
    "        print('开始读取五元组数据...')\n",
    "        \n",
    "        for FileName in tqdm(self.FILE_PATH,desc='五元组文件'):\n",
    "            #跳过非csv文件\n",
    "            if not FileName.endswith('.Quintuple.csv'):\n",
    "                continue\n",
    "            try:\n",
    "                #获取真实路径\n",
    "                file_path = Path + '/' + FileName\n",
    "                feature_path = self.FEATURE_PATH + '/' + FileName.split('.Quintuple.csv')[0] + '.features.csv'\n",
    "                f = open(file_path, 'r')\n",
    "                g = open(feature_path, 'r')\n",
    "                l1, l2 = f.readline(), g.readline()\n",
    "                print(file_path)\n",
    "                while l1 and l2:\n",
    "                    fields1 = l1.split(',')\n",
    "                    fields2 = l2.split('$')\n",
    "                    timestamp, size, eth_src, eth_dst = float(fields1[1]), int(fields1[2]), fields1[3], fields1[4]\n",
    "                    tcp_stream, udp_stream = fields2[-10], fields2[-4]\n",
    "                    yield timestamp, size, eth_src, eth_dst, tcp_stream, udp_stream\n",
    "                    l1, l2 = f.readline(), g.readline()\n",
    "                f.close()\n",
    "                g.close()\n",
    "                if l1 or l2:\n",
    "                    print('Error: line count does not match')\n",
    "                print('finish reading ..')\n",
    "            except:\n",
    "                print(f'错误:{file_path}')\n",
    "                print(f'Details:出错行_fields1:{fields1}_fields2{fields2}\\n')\n",
    "    \n",
    "    '''函数：数据集生成(1)'''\n",
    "    def get_dataset(self,Path):\n",
    "        \n",
    "        print('开始构造数据集...')\n",
    "        \n",
    "        dataset = {mac: [] for mac in self.device_list.values()}\n",
    "        flow_set = {mac: {} for mac in self.device_list.values()}\n",
    "\n",
    "        flow_instance_duration = 10  # seconds\n",
    "        flow_duration = 2  # seconds\n",
    "\n",
    "        def on_new_data_point(sample, mac):\n",
    "            total_count = len(sample['packet_series'])\n",
    "            packet_counter = {}\n",
    "            for p in sample['packet_series']:\n",
    "                packet_counter[p] = packet_counter.get(p, 0) + 1\n",
    "            distribution = {pl: count / total_count for pl, count in packet_counter.items()}\n",
    "            # print(distribution)\n",
    "            dataset[mac].append({'t': sample['packet_time'][0], 'fv': distribution})\n",
    "\n",
    "        # flow_record {'start_time': int, 'last_time': int, packet_series: [], packet_time: []}\n",
    "        def on_new_packet(packet_tuple, timestamp, mac, stream_id):\n",
    "            if stream_id not in flow_set[mac]:\n",
    "                flow_set[mac][stream_id] = {\n",
    "                    'packet_series': [packet_tuple],\n",
    "                    'packet_time': [timestamp]\n",
    "                }\n",
    "            else:\n",
    "                if timestamp - flow_set[mac][stream_id]['packet_time'][0] > flow_instance_duration or \\\n",
    "                        timestamp - flow_set[mac][stream_id]['packet_time'][-1] > flow_duration:\n",
    "                    on_new_data_point(flow_set[mac][stream_id], mac)\n",
    "                    # sliding window\n",
    "                    while flow_set[mac][stream_id]['packet_time'] and \\\n",
    "                            (timestamp - flow_set[mac][stream_id]['packet_time'][0] > flow_instance_duration or\n",
    "                             timestamp - flow_set[mac][stream_id]['packet_time'][-1] > flow_duration):\n",
    "                        flow_set[mac][stream_id]['packet_series'] = flow_set[mac][stream_id]['packet_series'][1:]\n",
    "                        flow_set[mac][stream_id]['packet_time'] = flow_set[mac][stream_id]['packet_time'][1:]\n",
    "                flow_set[mac][stream_id]['packet_series'].append(packet_tuple)\n",
    "                flow_set[mac][stream_id]['packet_time'].append(timestamp)\n",
    "\n",
    "        for t, size, eth_src, eth_dst, tcp_stream, udp_stream in self.data_generator(Path):\n",
    "            if not tcp_stream and not udp_stream:\n",
    "                continue\n",
    "            stream_id = 't' + tcp_stream if tcp_stream else 'u' + udp_stream\n",
    "            if eth_src in self.device_list.values():\n",
    "                packet_label = (size, DIRECTION_OUT)\n",
    "                on_new_packet(packet_label, t, eth_src, stream_id)\n",
    "            if eth_dst in self.device_list.values():\n",
    "                packet_label = (size, DIRECTION_IN)\n",
    "                on_new_packet(packet_label, t, eth_dst, stream_id)\n",
    "        return dataset\n",
    "    \n",
    "    '''函数：存储数据集'''\n",
    "    def store_dataset(self):\n",
    "        \n",
    "        print('开始导出数据集...')\n",
    "        \n",
    "        #生成测试、训练集\n",
    "        train_set = self.get_dataset('./CsvHub/Quintuple/train-data')\n",
    "        test_set = self.get_dataset('./CsvHub/Quintuple/test-data')\n",
    "\n",
    "        simple_record = {k: set() for k in train_set.keys()}\n",
    "        valid_train_set = {k: [] for k in train_set.keys()}\n",
    "        for mac, instances in train_set.items():\n",
    "            for i in instances:\n",
    "                if len(i['fv']) <= 3:\n",
    "                    simple_record[mac].add(tuple(i['fv'].keys()))\n",
    "                else:\n",
    "                    valid_train_set[mac].append(i)\n",
    "\n",
    "        simple_record_path = './Data/simple.pkl'\n",
    "        train_flow_path = './Data/train-flow.pkl'\n",
    "        test_data_path = './Data/test-data.pkl'\n",
    "        pickle.dump(simple_record, open(simple_record_path, 'wb'))\n",
    "        pickle.dump(valid_train_set, open(train_flow_path, 'wb'))\n",
    "        pickle.dump(test_set, open(test_data_path, 'wb'))\n",
    "\n",
    "\n",
    "def normalize(sample):\n",
    "    sqrt2 = math.sqrt(2)\n",
    "    vector = []\n",
    "    for direction in [0x00, 0x01]:\n",
    "        for length in range(20, 1521):\n",
    "            vector.append(sample.get((length, direction), 0.0))\n",
    "    return list(map(lambda x: math.sqrt(x) / sqrt2, vector))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unsw_dataset = UNSWDataset()\n",
    "    unsw_dataset.store_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （三）基于LSH的查询器的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 局部敏感性哈希（LSH）查询器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import falconn\n",
    "import numpy as np\n",
    "\n",
    "class LSHClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.mac_index_map = {}\n",
    "        self.sample_instance_set = []\n",
    "        self.lsh_index = None\n",
    "\n",
    "    def load_training_data(self, training_data):\n",
    "        start_index = 0\n",
    "        for device_mac, instances in training_data.items():\n",
    "            self.mac_index_map[device_mac] = (start_index, start_index + len(instances))\n",
    "            start_index += len(instances)\n",
    "            self.sample_instance_set.extend(instances)\n",
    "\n",
    "        parameter = falconn.get_default_parameters(len(self.sample_instance_set), len(self.sample_instance_set[0]))\n",
    "        self.lsh_index = falconn.LSHIndex(parameter)\n",
    "        self.lsh_index.setup(np.array(self.sample_instance_set, dtype=np.float32))\n",
    "\n",
    "    def get_classification_result(self, sample):\n",
    "        q = self.lsh_index.construct_query_object()\n",
    "        index = q.find_nearest_neighbor(np.array(sample, dtype=np.float32))\n",
    "        for k, v in self.mac_index_map.items():\n",
    "            left_bound, right_bound = v\n",
    "            if left_bound <= index < right_bound:\n",
    "                return k, index\n",
    "        print(\"fail to get classification result, index = \", index)\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_set = pickle.load(open('./Data/train-flow.pkl', 'rb'))\n",
    "print(train_set.keys())\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_hellinger_distance(v1, v2):\n",
    "    s1 = set(v1.keys())\n",
    "    s2 = set(v2.keys())\n",
    "    s = s1 | s2\n",
    "    d = 0.0\n",
    "    for packer_header in s:\n",
    "        p1 = v1.get(packer_header, 0.0)\n",
    "        p2 = v2.get(packer_header, 0.0)\n",
    "        d += (math.sqrt(p1) - math.sqrt(p2)) ** 2\n",
    "    d = math.sqrt(d) / math.sqrt(2)\n",
    "    return d\n",
    "\n",
    "\n",
    "def merge_sample(v1, v2, w):\n",
    "    S = {}\n",
    "    s1 = set(v1.keys())\n",
    "    s2 = set(v2.keys())\n",
    "    s = s1 | s2\n",
    "    for packet_header in s:\n",
    "        p1 = v1.get(packet_header, 0.0)\n",
    "        p2 = v2.get(packet_header, 0.0)\n",
    "        p = (p1 * w + p2) / (w + 1)\n",
    "        S[packet_header] = p\n",
    "    return S\n",
    "        \n",
    "\n",
    "# data reduction\n",
    "\n",
    "new_dataset = {}\n",
    "for k, v in train_set.items():\n",
    "    sample_space_map = {}\n",
    "    total_count = len(v)\n",
    "    for instance in v:\n",
    "        if tuple(instance['fv'].keys()) in sample_space_map:\n",
    "            original_bucket = sample_space_map[tuple(instance['fv'].keys())]\n",
    "            bucket, found = [], False\n",
    "            for index, (sample, n) in enumerate(original_bucket):\n",
    "                d = calculate_hellinger_distance(instance['fv'], sample)\n",
    "                if d < 1e-3:\n",
    "                    new_sample = merge_sample(sample, instance['fv'], n)\n",
    "                    bucket.append((new_sample, n + 1))\n",
    "                    found = True\n",
    "                    bucket.extend(original_bucket[index + 1:])\n",
    "                    break\n",
    "                else:\n",
    "                    bucket.append((sample, n))\n",
    "            if not found:\n",
    "                bucket.append((instance['fv'], 1))\n",
    "            sample_space_map[tuple(instance['fv'].keys())] = bucket\n",
    "        else:\n",
    "            sample_space_map[tuple(instance['fv'].keys())] = [(instance['fv'], 1)]\n",
    "    print(k, total_count, len(sample_space_map))\n",
    "    new_dataset[k] = sample_space_map\n",
    "    \n",
    "pickle.dump(new_dataset, open('./Data/new-train-flow.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import data\n",
    "import lsh_classifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "def offline_verification_unsw_new():\n",
    "    unsw_dataset = data.UNSWDataset()\n",
    "    classifier = lsh_classifier.LSHClassifier()\n",
    "    \n",
    "    # get training dataset\n",
    "    print('开始进行模型训练...')\n",
    "    flow_data_path = './Data/new-train-flow.pkl'\n",
    "    simple_record_path = './Data/simple.pkl'\n",
    "    training_set = pickle.load(open(flow_data_path, 'rb'))\n",
    "    simple_record = pickle.load(open(simple_record_path, 'rb'))\n",
    "    normalized_dataset = {k: [] for k in training_set.keys()}\n",
    "    for mac, instances in tqdm(training_set.items()):\n",
    "        if mac in unsw_dataset.iot_list.values():\n",
    "            for samples in instances.values():\n",
    "                for s in samples:\n",
    "                    fv, _ = s\n",
    "                    normalized_dataset[mac].append(data.normalize(fv))\n",
    "    classifier.load_training_data(normalized_dataset)\n",
    "    \n",
    "    #测试集\n",
    "    print('开始运行测试集...')\n",
    "    test_data_path = './Data/test-data.pkl'\n",
    "    test_set = pickle.load(open(test_data_path, 'rb'))\n",
    "    for mac, test_instances in tqdm(test_set.items()):\n",
    "        result_record_path = './QueryResult/' + mac.replace(':', '-') + '.txt'\n",
    "        f = open(result_record_path, 'w')\n",
    "        device_simple_record = simple_record[mac]\n",
    "        for test_instance in test_instances:\n",
    "            if len(test_instance['fv']) <= 3:\n",
    "                if tuple(test_instance['fv'].keys()) in device_simple_record:\n",
    "                    f.write(\"{} {}\\n\".format(test_instance['t'], 0))\n",
    "                else:\n",
    "                    f.write(\"{} {}\\n\".format(test_instance['t'], -1))\n",
    "            else:\n",
    "                result, index = classifier.get_classification_result(data.normalize(test_instance['fv']))\n",
    "                if result != None:\n",
    "                    label = unsw_dataset.label_map[result]\n",
    "                    f.write(\"{} {} {}\\n\".format(test_instance['t'], label, index))\n",
    "                else:\n",
    "                    f.write(\"{} {}\\n\".format(test_instance['t'], -2))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    offline_verification_unsw_new()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （四）数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  测试集命中率计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d0-c5-d3-28-c9-2a    *命中率： 99.446 %\n",
      "50-c7-bf-33-1f-09    *命中率： 97.259 %\n",
      "94-10-3e-36-60-09    *命中率： 99.61 %\n",
      "5c-41-5a-ce-36-02    *命中率： 100.0 %\n",
      "28-c2-dd-47-17-b6    *命中率： 99.997 %\n",
      "64-bc-0c-2a-d2-aa    *命中率： 91.015 %\n",
      "f4-84-4c-45-69-07    *命中率： 99.64 %\n",
      "90-8d-78-e3-81-0c    *命中率： 99.995 %\n",
      "64-16-66-1f-f0-3e    *命中率： 99.46 %\n",
      "d0-c5-d3-90-54-73    *命中率： 99.949 %\n",
      "50-c7-bf-59-d5-84    *命中率： 99.829 %\n",
      "c4-12-f5-de-38-20    *命中率： 99.992 %\n",
      "00-17-88-69-ee-e4    *命中率： 99.93 %\n",
      "\n",
      "平均命中率： 98.932 %\n"
     ]
    }
   ],
   "source": [
    "Accuracys = {}\n",
    "import os\n",
    "for root,dirs,file in os.walk('./QueryResult'):\n",
    "    for each in file:\n",
    "        if each.endswith('.txt') and each.find('checkpoint') == -1:\n",
    "            FilePath = root + '/' + each\n",
    "            No_Hit = 0\n",
    "            Count = 0\n",
    "            with open(FilePath,'r') as f:\n",
    "                ResultList = f.read().strip().split('\\n')\n",
    "                for Result in ResultList:\n",
    "                    Result = Result.split()\n",
    "                    #print(Result)\n",
    "                    Count += 1\n",
    "                    if (int(Result[-1]) == -1 or int(Result[-1]) == -2):\n",
    "                        No_Hit += 1\n",
    "                \n",
    "                f.close()\n",
    "            \n",
    "                Accuracys[each.split('.txt')[0]] = (Count-No_Hit)/Count\n",
    "                print(each.split('.txt')[0],'   *命中率：',round(Accuracys[each.split('.txt')[0]]*100,3),'%')\n",
    "                \n",
    "# 平均命中率\n",
    "AvgAcc = 0\n",
    "for Devices in Accuracys:\n",
    "    AvgAcc = AvgAcc + Accuracys[Devices]/len(Accuracys)\n",
    "print('\\n平均命中率：',round(AvgAcc*100,3),'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
